\documentclass{article}
\usepackage{vs}
\begin{document}

\lecture{XX}{Probability Recap}{Edo Liberty and Matthijs Douze}

\section{Preliminaries}
A variable $X$ is a random variable if it assumes different values
according to a probability distribution. For example, $X$ can 
denote the outcome of a three sided die throw. 
The variable $X$ takes the values $x = 1,2,3$ with equal probabilities. 

The expectation of $X$ is the sum over the possible values times the probability of the events.
\begin{equation}
\E[X] = \sum_{x=1}^{3}x \Pr(X = x)=
1\frac{1}{3}+2\frac{1}{3}+3\frac{1}{3} = 2
\end{equation}


Continuous random variable are described by their distribution function $\varphi$.
$$
\Pr[Y \in [a,b]] = \int_{a}^{b}\varphi (t) dt.
$$
For a function $\varphi$ to be a valid distribution we must have:
\begin{eqnarray}
\forall \;t, \;\; \varphi(t) &\ge& 0  \mbox{\;\;\; (where it is defined)}\\
\int_{a}^{b}\varphi (t) dt && \mbox{is well defined for all $a$ and $b$}\\
\int_{-\infty}^{-\infty}\varphi (t) dt &=& 1
\end{eqnarray}

For example consider the continuous variable $Y$ taking values in
$[0,1]$ uniformly. That means $\varphi(t) = 1$ if $t \in [0,1]$ and zero else.
This means that the probability of $Y$ being in the interval $[t,t + dt]$ is exactly $dt$. And so the expectation of $Y$ is:
\begin{equation}
\E[Y] = \int_{t=0}^{1}t \varphi(t) dt = \int_{t=0}^{1}t \cdot dt = \frac{1}{2}t^2|_{0}^{1} = 1/2
\end{equation}

\begin{remark}
Strictly speaking, distributions are not necessarily continuous or bounded functions. 
In fact, they can even no be a function at all. 
For example, the distribution of $X$ above includes three Dirac-$\delta$ functions which are not valid functions.
\end{remark}


\subsection{Dependence and Independence}
A variable $X$ is said to be {\it dependent} on $Y$ if the distribution of $X$ given $Y$ is different than the distribution of $X$. 
For example. Assume the variable $X$ takes the value $1$ if $Y$ takes a
value of less than $1/3$ and the values $2$ or $3$ with equal probably otherwise ($1/2$ each).
%
Clearly, the probability of $X$ assuming each of its values is still
$1/3$. however, if we know that $Y$ is $0.7234$ the probability of
$X$ assuming the value $1$ is zero. Let us calculate the expectation of $X$ given $Y$ as an exercise.
\begin{eqnarray}
\E(X | Y) = \sum_{x=1}^{3} x \Pr(X = x | Y \le 1/3) = 1\cdot 1\\
\E(X | Y) = \sum_{x=1}^{3} x \Pr(X = x | Y > 1/3) = 1\cdot 0 + 2
\frac{1}{2} + 3\frac{1}{2}  = 2.5
\end{eqnarray}
$E(X | Y) = 1$ for $y \in [0,1/3]$ and $E(X | Y) = 2.5$ for $y \in (1/3,1]$.\\
Remember: $\E(X | Y)$ is a function of $y$!

\begin{definition}[Independence]
Two variables are said to be {\it Independent} if:
\[
\forall y,\;\;\Pr[ X=x | Y = y] = \Pr[X=x].
\]
They are {\it dependent} otherwise.
\end{definition}


\begin{fact}
If two variables $X$ and $Y$ are {\it Independent} the so are $f(X)$ and $g(Y)$ for any functions $f$ and $g$.
\end{fact}


\begin{fact}[Linearity of expectation 1]%
For any random variable and any constant $\alpha$:
\begin{equation}
\E[\alpha X] = \alpha \E[X]
\end{equation}
\end{fact}

\begin{fact}[Linearity of expectation 2]%
For any two random variables
\begin{equation}
\E_{X,Y}[X+Y] = \E[X] + \E[Y]
\end{equation}
even if they are dependent.
\end{fact}


\begin{fact}[Multiplication of random variables]%
For any two {\bf independent} random variables
\begin{equation}
\E_{X,Y}[XY] = \E[X]\E[Y]
\end{equation}
This does not necessarily hold if they are dependent.
\end{fact}

\begin{definition}[Variance]%
For a random variable $X$ we have 
\begin{equation}
\Var[X] = \E[(X - \E[X])^2] = \E[X^2] - (\E[X])^2
\end{equation}
The standard deviation $\sigma$ of $X$ is defined to be $\sigma(X) \equiv \sqrt{\Var[X]}$.
\end{definition}

\begin{definition}[Additivity of variances]%
For any two {\bf independent} variables $X$ and $Y$ we have 
\begin{equation}
\Var[X + Y] = \Var[X] + \Var[Y]
\end{equation}
\end{definition}

\begin{fact}[Markov's inequality]%
For any {\it positive} random variable $X$:
\begin{equation}
\Pr(X > t) \le \frac{E[X]}{t}
\end{equation}
\end{fact}

\begin{fact}[Chebyshev's inequality]%
For any random variable $X$
\begin{equation}
\Pr[|X-E[X]| > t] \le \frac{\sigma^2(X)}{t^2}
\end{equation}
\end{fact}


\end{document}


%%%%%%%%
