\documentclass{article}
\usepackage{vs}
\begin{document}

\lecturetitle{Class 5 - Dimensionality Reduction}

\section{Random-projection}

We will give a simple proof of the following, rather amazing, fact. Every set of $n$ points 
in a Euclidian space (say in dimension $d$) can be embedded into the Euclidean space of 
dimension $k = O(\log(n)/\eps^2)$ such that all pairwise distances are preserved up distortion $1\pm \eps$.
We will prove the construction of \cite{DasGuptaGupta99} which is simpler than the one in \cite{JL84}.


We will argue that a certain distribution over the choice of a matrix $\R \in \R^{k \times d}$ gives that:
\begin{equation}
\label{e1}
\forall x \in \Sph^{d-1} \;\; \Pr\left[ \left| ||\frac{1}{\sqrt{k}}Rx|| - 1 \right| > \eps \right] \le \frac{1}{n^2} 
\end{equation}
Before we pick this distribution and show that Equation~\ref{e1} holds for it, let us first see
that this gives the opening statement. 

Consider a set of $n$ points $x_1,\ldots, x_n$ in Euclidean space $\R^d$. Embedding these points
into a lower dimension while preserving all distances between
them up to distortion $1\pm \eps$ means approximately preserving the norms of all 
${n \choose 2}$ vectors $x_i - x_j$. Assuming Equation~\ref{e1} holds and using the union 
bound, this property will fail to hold for at least one $x_i - x_j$ pair with probability at most ${n \choose 2}\frac{1}{n^2} \le 1/2$.
Which means that all ${n \choose 2}$ point distances are preserved up to distortion $\eps$ with probability at least $1/2$.


\section{Matrices with normally distributed independent entries}
We consider the distribution of matrices $R$ such that each $R(i,j)$ is drawn independently from  a
normal distribution with mean zero and variance $1$, $R(i,j) \sim \N(0,1)$. We show that for this
distribution Equation~\ref{e1} holds for some $k \in O(\log(n)/\eps^2)$.

First consider the random variable $z = \sum_{j=1}^{d}r(j)x(j)$ where $r(j) \sim \N(0,1)$. 
To understand how the variable $z$ distributes we recall the two-stability of the
normal distribution. Namely, if $z_3 = z_2 + z_1$ and 
$z_1 \sim \N(\mu_1,\sigma_{1})$ and $z_2 \sim \N(\mu_2,\sigma_{2})$ then, $$z_3 \sim \N(\mu_1 + \mu_2,\sqrt{\sigma^{2}_{1} + \sigma^{2}_{2}}).$$
In our case,  $r(i)x(i) \sim \N(0,x_{i})$ and therefore, $z = \sum_{i=1}^{d}r(i)x(i) \sim \N(0,\sqrt{\sum_{i=1}^{d}x^{2}_{i}}) \sim \N(0,1)$.
%
Now, note that each element in the vector $Rx$ distributes exactly like $z$.
Defining $k$ identical copies of $z$, $z_1,\ldots,z_k$,
We get that $||\frac{1}{\sqrt{k}}Rx||$ distributes exactly like $\sqrt{\frac{1}{k}\sum_{i=1}^{k} z^{2}_{i}}$.
Thus, proving Equation~\ref{e1} reduces to showing that:
\begin{equation}
\Pr\left[ \left| \sqrt{\frac{1}{k}\sum_{i=1}^{k} z^{2}_{i}} - 1 \right| > \eps \right] \le \frac{1}{n^2} 
\end{equation}
for a set of independent normal random variables $z_1,\ldots,z_k \sim \N(0,1)$.
It is sufficient to demanding that $\Pr[\sum_{i=1}^{k} z^{2}_{i} \ge k(1+\eps)^2]$ and $\Pr[\sum_{i=1}^{k} z^{2}_{i} \le k(1-\eps)^2]$ are both smaller than $1/2n^2$.
We start with bounding the probability that $\sum_{i=1}^{k} z^{2}_{i} \ge k(1+\eps)$ (this is okay because $k(1+\eps) < k(1+\eps)^2$).
\[
\Pr[\sum z^{2}_{i} \ge k(1+\eps)] = \Pr[e^{\lambda \sum z^{2}_{i}} \le e^{\lambda k (1+\eps)}] \le (\E[e^{\lambda z^2}])^k/e^{\lambda k (1+\eps)}
\]
Since $z \sim \N(0,1)$ we can compute $\E[e^{\lambda z^2}]$ exactly:
\[
\E [e^{\lambda z^{2}}] = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{\lambda t^{2}} e^{-\frac{t^{2}}{2}} dt =\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac{(t\sqrt{1-2\lambda})^{2}}{2}}dt = e^{\frac{1}{2} \log(1-2\lambda)}
\]
The final step is by substituting $t' = t\sqrt{1-2\lambda}$ and recalling that $\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac{t'^{2}}{2}}dt' = 1$.
Finally, using the fact that $log(\frac{1}{1-2\lambda}) \le 2\lambda + 4\lambda^2$ for $\lambda \in [0,1/4]$ we have:
\[
\E [e^{\lambda z^{2}}] = \frac{1}{\sqrt{1-2\lambda}} = e^{\frac{1}{2} \log(\frac{1}{1-2\lambda})} \le e^{\lambda + 2\lambda^2}
\]
Substituting this into the equation above we have that:
\[
\Pr \le e^{k(\lambda  + 2\lambda^2) - k\lambda (1+\eps)} = e^{ 2k\lambda^2 - k\lambda\eps}  = e^{ - k\eps^2/8}  
\]
for $\lambda \leftarrow \eps/4$. Finally, our condition that 
\[
\Pr[\sum_{i=1}^{k} z^{2}_{i} \ge k(1+\eps)] \le e^{ - k\eps^2/8} \le 1/2n^2
\]
is achieved by $k = c\log(n)/\eps^2$.
Calculating for $\Pr[\sum_{i=1}^{k} z^{2}_{i} \le k(1-\eps)]$ in the same manner shows that $k = c\log(n)/\eps^2$ is also sufficient for this case.
This completes the proof.





\section{Fast Random Projections}
We discussed in class the fact that random projection matrices cannot be made sparse in general.
That is because projecting sparse vectors and preserving their norm requires the projecting matrix is almost fully dense see also \cite{JelaniH2012} and \cite{KaneN12}.

But, the question is, can we actively make sure that $x$ is not sparse? If so, can we achieve a sparse random projection for non sparse vectors?
These two questions received a positive answer in the seminal work by Ailon and Chazelle \cite{AilonCh06}.
The results of \cite{AilonCh06} were improved and simplified over the years. See \cite{AilonL11} for the latest result and an overview.

In this lesson we will produce a very simple algorithm based on the ideas in \cite{AilonCh06}.
This algorithm will require a target dimension of $O(\log^2(n)/\eps^2)$ instead of $O(\log(n)/\eps^2)$ but will be much simpler to prove.

\subsection{Fast vector $\ell_4$ norm reduction}
The goal of this subsection is to devise a linear mapping which preserves vector's $\ell_2$ norms but reduces their $\ell_4$ norms with high probability.
This will work to our advantage because, intuitively, vectors whose $\ell_4$ norm is small cannot be too sparse.
For this we will need to learn what Hadamard matrices are.

Hadamard matrices are commonly used in coding theory and are conceptually
close for Fourier matrices. We assume for convenience that $d$ is a power of $2$ (otherwise we can pad out vectors with zeros).
The Walsh Hadamard transform of a vector $x \in \R^{d}$ is the
result of the matrix-vector multiplication $Hx$ where $H$ is a $d
\times d$ matrix whose entries are $H(i,j) = \frac{1}{\sqrt{d}}(-1)^{\langle
i,j\rangle}$. Here ${\langle i,j\rangle}$ means the dot product over
$F_2$ of the bit representation of $i$ and $j$ as binary vectors of
length $\log(d)$.
Another way to view this is to define Hadamard Matrices recursively.
\begin{equation*} %
H_{1} = \frac{1}{\sqrt{2}}\left(
          \begin{array}{rr}
            1 & 1 \\
            1 & -1\\
          \end{array}
        \right)
,\;\;
        H_{d} = \frac{1}{\sqrt{2}}\left(
          \begin{array}{r:r}
            H_{d/2} & H_{d/2} \\ \hdashline
            H_{d/2} & -H_{d/2}\\
          \end{array}
        \right)
\end{equation*} %
Here are a few interesting (and easy to show) facts about Hadamard matrices.
\begin{enumerate}
\item $H_d$ is a unitary matrix $\|Hx\| = \|x\|$ for any vector $x\in \R^d$.
%\item $H_{d}(i,j) \in \{ \frac{1}{\sqrt{d}},- \frac{1}{\sqrt{d}}\}$
\item Computing $x \mapsto Hx$ requires $O(d\log(d))$ operations.
\end{enumerate}


We also define a diagonal matrix $D$ to be such that $D(i,i) \in \{1,-1\}$ uniformly.
Clearly, we have that $\|HDx\|_2 = \|x\|_2$ since both $H$ and $D$ are isotropies.
Let us now bound $\|HDx\|_\infty$.
$(HDx)(1) = \sum_{i=1}^{d}H(1,i)D(i,i) x_i = \sum_{i=1}^{d}\frac{x_i}{\sqrt{d}}s_i$ where $s_i \in \{-1,1\}$ uniformly.
To bound this we recap Hoeffding's inequality.
\begin{fact}[Hoeffding's inequality]
Let $X_1,\ldots,X_n$ be independent random variables s.t. $X_i \in [a_i,b_i]$.
Let $X = \sum_{i=1}^{n} X_i$.
\begin{equation}
\Pr[|X - \E[X]| \ge t] \le 2e^{-\frac{2 t^2}{\sum_{i=1}^{n} (b_i -a_i)^2}}
\end{equation}
\end{fact}
Invoking Hoeffding's inequality and then the union bound we get that if $\|HDx\|_\infty \le \sqrt{\frac{c \log(n)}{d}}$ for all points $x$.
Remark, for this we assumed $\log(d) = O(\log(n))$ otherwise we should have had $\log(nd)$ in the bound. 
The situation, however, that the dimension is super polynomial in the number of points is unlikely. 
Usually it is common to have $n > d$.

\begin{lemma}
Let $x \in \R^d$ by such that $\|x\|=1$. Then:
\[
\|HDx\|^4_4 = O(log(n)/d)
\]
with probability at least $1-1/\poly(n)$
\end{lemma}
\begin{proof}
Let us define $y = HDx$ and $z_i$ = $y_i^2$. 
From the above we have that $z_i \le \frac{c \log(n)}{d} = \eta$ with probability at least $1-1/\poly(n)$.
The quantity $\|HDx\|^4_4 = \|y\|_{4}^{4} = \sum_{i}z_i^2$ is a convex function of the $z$ variables which is defined over a polytop $z_i \in [0,1]$ and $\sum_{i} z_i = 1$ (this is because $\|y\|_2^2 = 1$).
This means that its maximal value is obtained on an extreme point of this polytope. 
In other words, the point $z_1,\ldots,z_{1/\eta} = \eta$ and $z_{1/\eta+1},\ldots,z_{d} = 0$ or $z = [\eta,\eta,\ldots,\eta,\eta,0,0,0,\ldots,0,0,0]$.
Computing the value of the function in this point gives $\sum_{i}z_i^2 \le (1/\eta)\cdot (\eta^2) = \eta$. Recalling the $\eta = \frac{c \log(n)}{d}$ completes the proof.  
\end{proof}

\subsection{Sampling from vectors with low $\ell_4$ norms}
Here we prove a very simple fact. For vectors whose $\ell_4$ is low, dimensionality reduction can be obtained by sampling.


Let $y$ be a vector such that $\|y\|_2 = 1$. Let $z$ be a sampled version of $y$ such that $z_i = y_i/\sqrt{p}$ with probability $p$ and $0$ else. 
This is akin to sampling, in expectation, $d\cdot p$ coordinates from $y$ (and scaling them up by $1/\sqrt{p}$).
Note the $\E[\|z\|^2] = \E[\|y\|^2] = 1$ moreover:
\[
\Pr[|\|z\|^2 - 1| > \eps] = \Pr[|\sum z_i^2 - 1| > \eps] = \Pr[|\sum b_i y_i^2/p - 1| > \eps]
\]
Where $b_i$ are independent random indicator variables taking the $b_i = 1$ with probability $p$ and $b_i = 0$ else.
To apply Chernoff's bound we must assert that $y_i^2/p \le 1$. Let's assume this for now and return to it later.
Applying Chernoff's bound we get
\[
\Pr[|\sum b_i y_i^2/p - 1| > \eps] \le e^{-\frac{c\eps^2}{\sigma^2}}
\]
where $\sigma^2 = \sum_{i} \E[(b_i y_i^2/p)^2] = \|y\|_{4}^{4}/p$.
Concluding that
\[
\Pr[|\|z\|^2 - 1| > \eps] \le e^{-\frac{cp\eps^2}{\|y\|_4^4}}
\]
This shows that the concentration of the sampling procedure really depends directly on the $\ell_4$ norm of the sampled vector.
If we plug in the bound on $\|y\|_4^4 = \|HDx\|_4^4$ from the previous section we get 
\[
\Pr[|\|z\|^2 - 1| > \eps] \le e^{-\frac{cp\eps d}{\log(n)}} \le \frac{1}{\poly(n)}
\]
For some $p \in O(\log^2(n)/d\eps^2)$. 

\subsection{Random Projection by Sampling}
Putting it all together we obtain the following.
\begin{lemma}
Define the following matrices
\begin{itemize}
\item $D$: A diagonal matrix such that $D_{i,i} \in \{+1,-1\}$ uniformly.
\item $H$: The $d\times d$ Walsh Hadamard Transform matrix.
\item $P$: A `sampling matrix' which contains each row of matrix $I_d\cdot \sqrt{p}$ with probability $p= c\log^2(n)/d\eps^2$.
\end{itemize}
Then, with at least constant probability the following holds.
\begin{enumerate}
\item The target dimension of the mapping is $k = c\log^2(n)/\eps^2$ (a factor $log(n)$ worse than optimal).
\item The mapping $x \mapsto PHDx$ is a $(1\pm\eps)$-distortion mapping for any set of $n$ points. 
That is, for any set $x_1,\ldots,x_n \in \R^d$ we have
\[
\|x_i -x_j\|(1-\eps) \le \|PHDx_i  - PHDx_j\| \le \|x_i -x_j\|(1+\eps)
\]
\item Storing $PHD$ requires at most $O(d + k\log(d))$ space.
\item Applying the mapping $x \mapsto PHDx$ requires at most $d\log(d)$ floating point operations.
\end{enumerate}
\end{lemma}


\bibliographystyle{plain}
\bibliography{vs}

\end{document}
%%%%%%%%
